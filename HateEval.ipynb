{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HateEval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hatefap/CrackingTheInterview/blob/master/HateEval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmtMYYbwf6wn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570fea61-5e40-48e5-a5e7-5d74bc6eeda6"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=8403cd4e2d417d5bfcbd8dbcf559f49f2f2c1e2917bee8b014be30c3678e15ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fhFe35xgT5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432416af-151a-462c-b465-3e2be54ff835"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paZMf6cegtcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8eb762-7005-4bff-ceb8-5eed6760e5b6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE5ZG_EQhLFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2be260ca-5dad-45a8-d7d7-b5ea62b9a1dd"
      },
      "source": [
        "cd /content/drive/My Drive/HateSpeechDetection/Dataset/Hate/No-Prc/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/HateSpeechDetection/Dataset/Hate/No-Prc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgtAzLxThMr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ac48ec-32c5-4f67-d59e-8c7f5fd64b9c"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HateEval  OffenseEval  single_runs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgJMRxPcrgrY"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7eF6M7ErQzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8950a92e-2cd3-4260-c4c4-dad55ce93031"
      },
      "source": [
        "df = pd.read_csv('test.csv')\n",
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pcm5UcgZQgws",
        "outputId": "2d4aa472-0879-4b4b-ef00-aacbb3101caa"
      },
      "source": [
        "res = df[df[\"label\"]==1]\n",
        "len(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1260"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtoVnYD3JNS4"
      },
      "source": [
        "# def df2dl(df):\n",
        "#     df = df.dropna()\n",
        "#     df.text = df.text.apply(lambda x: \"[CLS] \" + x + ' [SEP]')\n",
        "#     tokenized_texts = df.text.apply(lambda x: tokenizer.tokenize(' '.join(x.split()[:MAX_LEN])))\n",
        "#     token_ids = tokenized_texts.apply(tokenizer.convert_tokens_to_ids)\n",
        "#     token_ids_matrix = np.array(token_ids.apply(lambda x: x[:MAX_LEN] + [0] * max(0, MAX_LEN - len(x))).tolist()).astype('int64')\n",
        "#     attention_matrix = (token_ids_matrix != 0).astype('float')\n",
        "#     dataset = TensorDataset(torch.tensor(token_ids_matrix), torch.tensor(attention_matrix), torch.tensor(np.array(df.label)))\n",
        "#     data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "#     return data_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUn1sMKjJKlc"
      },
      "source": [
        "# import pandas as pd\n",
        "# df = pd.read_csv('train.csv')\n",
        "# df.dropna(inplace=True)\n",
        "# label_counts = df.label.value_counts(normalize=True)\n",
        "# train_data_loader = df2dl(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d4GqA68IiWy"
      },
      "source": [
        "# from transformers import AutoConfig, AlbertModel\n",
        "# albert_xxlarge_configuration = AlbertConfig()\n",
        "# # Initializing an ALBERT-base style configuration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0u4Wx_F3G4f"
      },
      "source": [
        "# albert_base_configuration = AutoConfig(\n",
        "#       num_attention_heads=12,\n",
        "#       intermediate_size=3072,\n",
        "#   )\n",
        "\n",
        "#  # Initializing a model from the ALBERT-base style configuration\n",
        "# model = AlbertModel(albert_base_configuration)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxHh6EyY3G2k"
      },
      "source": [
        "# configuration = model.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1k0PWGx3SGE"
      },
      "source": [
        "# configuration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y-NZ6LaIZrT"
      },
      "source": [
        "            if(dataset_names[d_index][:4]=='Offe'):\n",
        "              offe_f_one.append(metrics_test[2])\n",
        "              offe_accuracy.append(metrics_test[3])\n",
        "            elif (dataset_names[d_index][:4]=='Hate'):\n",
        "              hate_f_one.append(metrics_test[2])\n",
        "              hate_accuracy.append(metrics_test[3])\n",
        "            else:\n",
        "              sent_f_one.append(metrics_test[2])\n",
        "              sent_accuracy.append(metrics_test[3])\n",
        "\n",
        "            if(len(hate_f_one)>20 and len(hate_f_one)>20):\n",
        "              # print(\"mohammad\",max(hate_f_one),max(offe_f_one))\n",
        "              if((hate_f_one[-1]<hate_f_one[-21]) and (offe_f_one[-1]<offe_f_one[-21])):\n",
        "                offe_index = offe_f_one.index(max(offe_f_one))\n",
        "                hate_index = hate_f_one.index(max(hate_f_one))\n",
        "                print(\"\\nearly stopping at {}\\n \\\n",
        "                        Offensive max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\\n \\\n",
        "                        Hate max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\"\\\n",
        "                        .format(i_epoch ,\n",
        "                                max(offe_f_one) , offe_accuracy[offe_index], offe_index+1\n",
        "                                max(hate_f_one) , hate_accuracy[hate_index], hate_index+1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuOfPlWL5YWn",
        "outputId": "34577cc2-e57c-49fa-e23c-7f1bee142070"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.5MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 13.1MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 12.2MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 11.6MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 8.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 7.3MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 9.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 7.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 7.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 7.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 7.7MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 7.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 7.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 7.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK44BXxgPILs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "8c7ecf0e-1977-4b2d-d718-7ec8eec36375"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import datetime, time\n",
        "from shutil import rmtree\n",
        "import transformers\n",
        "from torch.utils import tensorboard\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
        "from torch.utils.tensorboard.summary import hparams\n",
        "import transformers\n",
        "import tqdm\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# %%\n",
        "import argparse\n",
        " \n",
        "parser = argparse.ArgumentParser()\n",
        " \n",
        "# parser.add_argument('--dataset', default='OffenseEval')\n",
        "# parser.add_argument('--max_len', default=60, type=int)\n",
        "# parser.add_argument('--epochs', default=5, type=int)\n",
        "# parser.add_argument('--num_classes', default=2, type=int)\n",
        "# parser.add_argument('--batch', default=64, type=int)\n",
        "#\n",
        " \n",
        "# parser.add_argument('--dataset', default='Hate/Conditions/4-Remove_Mention_Sign/HateEval')\n",
        "parser.add_argument('--dataset', default='')\n",
        " \n",
        "parser.add_argument('--max_len', default=140, type=int)\n",
        "parser.add_argument('--epochs', default=100, type=int)\n",
        "parser.add_argument('--num_classes', default=2, type=int)\n",
        "parser.add_argument('--batch', default=8, type=int)\n",
        " \n",
        "# parser.add_argument('--dataset', default='Sarcasm')\n",
        "# parser.add_argument('--max_len', default=60, type=int)\n",
        "# parser.add_argument('--epochs', default=6, type=int)\n",
        "# parser.add_argument('--num_classes', default=2, type=int)\n",
        "# parser.add_argument('--batch', default=64, type=int)\n",
        " \n",
        "parser.add_argument('--ntransformer', default='Albert')\n",
        " \n",
        "parser.add_argument('--device', default='cuda')\n",
        "parser.add_argument('--weight', action='store_false')\n",
        "parser.add_argument('--lr', default=2e-8, type=float)\n",
        "parser.add_argument('--grad_clip', default=5., type=float)\n",
        "parser.add_argument('--seed', default=1, type=int)\n",
        "parser.add_argument('--notes', default=None)\n",
        "parser.add_argument('--tensorboard', action='store_false')\n",
        "args = parser.parse_known_args()\n",
        "print(args)\n",
        " \n",
        "# %%\n",
        "MAX_LEN = args[0].max_len\n",
        "N_EPOCHS = args[0].epochs\n",
        "NUM_CLASSES = args[0].num_classes\n",
        "BATCH_SIZE = args[0].batch\n",
        "EPSILON = 10e-13\n",
        "LR = args[0].lr\n",
        "# DATASET = 'Dataset/' + args[0].dataset\n",
        "DATASET = '' + args[0].dataset\n",
        "WEIGHT = args[0].weight\n",
        "GRAD_CLIP = args[0].grad_clip\n",
        "device = torch.device(args[0].device) if torch.cuda.is_available() else torch.device('cpu')\n",
        "TRANSFORMER = args[0].ntransformer\n",
        "notes = args[0].notes\n",
        " \n",
        "SEED = args[0].seed\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        " \n",
        "hyper_parameters = {'batch': BATCH_SIZE, 'lr': LR, 'weight': WEIGHT, 'transformer': TRANSFORMER, 'seed': SEED}\n",
        " \n",
        "# %%\n",
        "if TRANSFORMER == 'Albert':\n",
        "    # tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-en\")\n",
        "    # transformer = AutoModelForMaskedLM.from_pretrained(\"nghuyong/ernie-2.0-en\")\n",
        " \n",
        "    tokenizer = transformers.AlbertTokenizer.from_pretrained('albert-base-v2', do_lower_case=True)\n",
        "    transformer = transformers.AlbertModel.from_pretrained(\"albert-base-v2\")\n",
        "    \n",
        "    # tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-tiny\")\n",
        "    # transformer = AutoModelForMaskedLM.from_pretrained(\"nghuyong/ernie-tiny\")\n",
        " \n",
        "    # tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-large-en\")\n",
        "    # transformer = AutoModelForMaskedLM.from_pretrained(\"nghuyong/ernie-2.0-large-en\")\n",
        " \n",
        "    #   erine_configuration = AutoConfig()\n",
        "#  # Initializing an ALBERT-base style configuration\n",
        "#   erine_base_configuration = AutoConfig(\n",
        "#       hidden_size=768,\n",
        "#       num_attention_heads=12,\n",
        "#       intermediate_size=3072,\n",
        "#   )\n",
        " \n",
        "#  # Initializing a model from the ALBERT-base style configuration\n",
        "#   model = AlbertModel(albert_xxlarge_configuration)\n",
        " \n",
        "# # Accessing the model configuration\n",
        "#   configuration = model.config\n",
        " \n",
        " \n",
        " \n",
        "elif TRANSFORMER == 'DistilBert':\n",
        "    tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
        "    transformer = transformers.DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        " \n",
        "# %%\n",
        " \n",
        "if args[0].tensorboard:\n",
        "    time_string = (datetime.datetime.utcnow() + datetime.timedelta(seconds=12600)).replace(microsecond=0).isoformat('_')\n",
        "    hyper_parameters_string = '--'.join(\n",
        "        [k + '=' + str(v) for k, v in hyper_parameters.items()])\n",
        "    logdir = f'single_runs/{args[0].dataset}/{hyper_parameters_string}'\n",
        "    # if notes is not None:\n",
        "    #     logdir += notes\n",
        "    if os.path.exists(logdir):\n",
        "        # rmtree(logdir)\n",
        "        logdir += '/' + time_string\n",
        "    tb = tensorboard.SummaryWriter(log_dir=logdir)\n",
        " \n",
        " \n",
        "# %%\n",
        " \n",
        "def df2dl(df,MAX_LEN_N):\n",
        "    df = df.dropna()\n",
        "    df.text = df.text.apply(lambda x: \"[CLS] \" + x + ' [SEP]')\n",
        "    tokenized_texts = df.text.apply (lambda x: tokenizer.tok(' '.join(x.split()[:MAX_LEN_N])))\n",
        "    token_ids = tokenized_texts.apply(tokenizer.convert_tokens_to_ids)\n",
        "    token_ids_matrix = np.array(\n",
        "        token_ids.apply(lambda x: x[:MAX_LEN_N] + [0] * max(0, MAX_LEN_N - len(x))).tolist()).astype('int64')\n",
        "    attention_matrix = (token_ids_matrix != 0).astype('float')\n",
        "    dataset = TensorDataset(torch.tensor(token_ids_matrix), torch.tensor(attention_matrix),\n",
        "                            torch.tensor(np.array(df.label)))\n",
        "    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return data_loader\n",
        " \n",
        " \n",
        "df = pd.read_csv('HateEval/train.csv')\n",
        "df.dropna(inplace=True)\n",
        "label_counts = df.label.value_counts(normalize=True)\n",
        "train_data_loader = df2dl(df,78)\n",
        "df = pd.read_csv('HateEval/test.csv')\n",
        "test_data_loader = df2dl(df,140)\n",
        " \n",
        " \n",
        "# %%\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.transformer = transformer\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(768, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, NUM_CLASSES if NUM_CLASSES > 2 else 1)\n",
        "        )\n",
        " \n",
        "    def forward(self, batch_input, batch_mask):\n",
        "      print(\"start\", batch_input.shape)\n",
        "      xx = self.transformer(batch_input, batch_mask)\n",
        "      # print(\"part1 \\n\")\n",
        "      # print(xx[0][:, 0, :].shape)\n",
        "      xx = self.ff(xx[0][:, 0, :]).squeeze()\n",
        "      print(\"part2\", xx.shape)\n",
        "      return xx\n",
        " \n",
        " \n",
        "model = MyModel()\n",
        "model = model.to(device)\n",
        "# %%\n",
        "param_optimizer = list(model.named_parameters())\n",
        " \n",
        "# no_decay = ['bias', 'gamma', 'beta']\n",
        "# optimizer_grouped_parameters = [\n",
        "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "#      'weight_decay_rate': 0.01},\n",
        "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "#      'weight_decay_rate': 0.0}\n",
        "# ]\n",
        " \n",
        " \n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        " \n",
        " \n",
        "# %%\n",
        "def calc_confusion_matrix(y, y_hat):\n",
        "    with torch.no_grad():\n",
        "        if NUM_CLASSES > 2:\n",
        "            pred = y_hat.argmax(-1)\n",
        "        else:\n",
        "            pred = y_hat.gt(0)\n",
        " \n",
        "    m = confusion_matrix(y.cpu(), pred.cpu(), range(NUM_CLASSES))\n",
        "    return m\n",
        " \n",
        " \n",
        "def calc_metrics(m):\n",
        "    p = (m.diagonal() / m.sum(0).clip(EPSILON)).mean()\n",
        "    r = (m.diagonal() / m.sum(1).clip(EPSILON)).mean()\n",
        "    f1 = ((2 * p * r) / (p + r)).mean()\n",
        "    accu = m.diagonal().sum() / m.sum()\n",
        "    return p, r, f1, accu\n",
        " \n",
        " \n",
        "train_weights = (1 / label_counts)\n",
        "train_weights = train_weights.to_numpy() / train_weights.sum()\n",
        "train_weights = torch.Tensor(train_weights).to(device)\n",
        " \n",
        "if WEIGHT:\n",
        "    if NUM_CLASSES > 2:\n",
        "        criterion = nn.CrossEntropyLoss(reduction='sum', weight=train_weights)\n",
        "    else:\n",
        "        criterion = nn.BCEWithLogitsLoss(reduction='sum', pos_weight=train_weights[1])\n",
        "else:\n",
        "    if NUM_CLASSES > 2:\n",
        "        criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "    else:\n",
        "        criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
        " \n",
        "metrics_all = []\n",
        "f_one=[]\n",
        "accuracy=[]\n",
        "flag = True\n",
        "for i_epoch in range(1, N_EPOCHS + 1):\n",
        " \n",
        "    model.train()\n",
        " \n",
        "    # Tracking variables\n",
        "    train_loss = 0\n",
        "    train_confusion_table = np.zeros((NUM_CLASSES, NUM_CLASSES))\n",
        "    train_total = 0\n",
        "    metrics_train = ()\n",
        " \n",
        "    p_bar = tqdm(train_data_loader)\n",
        "    for step, batch in enumerate(p_bar):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        batch_input, batch_mask, batch_y = batch\n",
        "        optimizer.zero_grad()\n",
        " \n",
        "        # print(\"for\")\n",
        "        # print(batch_input.shape)\n",
        "        # print(batch_mask.shape)\n",
        "        # break\n",
        "        logits = model.forward(batch_input, batch_mask)\n",
        "        # break\n",
        "        \n",
        "        loss = criterion(logits, batch_y if NUM_CLASSES > 2 else batch_y.float())\n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        assert grad_norm >= 0, 'encounter nan in gradients.'\n",
        "        optimizer.step()\n",
        " \n",
        "        train_confusion_table += calc_confusion_matrix(batch_y, logits)\n",
        "        train_total += batch_input.size(0)\n",
        "        train_loss += loss.item()\n",
        " \n",
        "        metrics_train = (\n",
        "            train_loss / train_total,\n",
        "            *calc_metrics(train_confusion_table),\n",
        "        )\n",
        " \n",
        "        p_bar.set_description(\n",
        " \n",
        "            '[ EP {0:02d} ]'\n",
        "            '[ TRN LS: {1:.4f} PR: {2:.4f} F1: {4:.4f} AC: {5:.4f}]'\n",
        "                .format(i_epoch, *metrics_train))\n",
        " \n",
        "    if args[0].tensorboard:\n",
        "        for k, v in zip(['train_loss', 'train_precision', 'train_recall', 'train_f1', 'train_accuracy'], metrics_train):\n",
        "            tb.add_scalar(k, v * 100, global_step=i_epoch)\n",
        " \n",
        "    with torch.no_grad():\n",
        "        # Set our model to testing mode (as opposed to evaluation mode)\n",
        "        model.eval()\n",
        " \n",
        "        # Tracking variables\n",
        "        # test_loss = 0\n",
        "        test_confusion_table = np.zeros((NUM_CLASSES, NUM_CLASSES))\n",
        "        test_total = 0\n",
        " \n",
        "        # test the data for one epoch\n",
        "        p_bar = tqdm(test_data_loader)\n",
        "        for step, batch in enumerate(p_bar):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            batch_input, batch_mask, batch_y = batch\n",
        " \n",
        "            logits = model(batch_input, batch_mask)\n",
        " \n",
        "            test_confusion_table += calc_confusion_matrix(batch_y, logits)\n",
        "            test_total += batch_input.size(0)\n",
        "            # test_loss += loss.item()\n",
        " \n",
        "            metrics_test = (\n",
        "                # test_loss / test_total,\n",
        "                *calc_metrics(test_confusion_table),\n",
        "            )\n",
        "            \n",
        "            p_bar.set_description(\n",
        "                '[ EP {0:02d} ]'\n",
        "                '[ TST F1: {3:.4f} AC: {4:.4f}]'\n",
        "                    .format(i_epoch, *metrics_test))\n",
        "            \n",
        "    f_one.append(metrics_test[2])\n",
        "    accuracy.append(metrics_test[3])\n",
        "    # print(len(patience) , patience[-1])\n",
        "    if(len(f_one)>20):\n",
        "      if(f_one[-1]<f_one[-21]):\n",
        "        index = f_one.index(max(f_one))\n",
        "        print(\"\\nearly stopping at {} , max F1 is {:.4f} , max accuracy is {:.4f}\\\n",
        "         , in epoch {}\".format(i_epoch , max(f_one) , accuracy[index], index+1))\n",
        "        \n",
        "        a = \"\\nearly stopping at {} , max F1 is {:.4f} , max accuracy is  {:.4f} ,\\\n",
        "         in epoch {}\".format(i_epoch , max(f_one) , accuracy[index], index+1)\n",
        " \n",
        "        with open(\"hateResult.txt\", 'w') as writer:\n",
        "          writer.write(a)\n",
        "        flag = False\n",
        "        break           \n",
        " \n",
        "    if args[0].tensorboard:\n",
        "        for k, v in zip(['test_precision', 'test_recall', 'test_f1', 'test_accuracy'], metrics_test):\n",
        "            tb.add_scalar(k, v * 100, global_step=i_epoch)\n",
        " \n",
        "    metrics_all.append((*metrics_train, *metrics_test))\n",
        " \n",
        "# metrics = np.array(metrics_all).T\n",
        "# best_index = metrics[-2].argmax()\n",
        "# best_metrics = metrics.T[best_index]\n",
        " \n",
        "# if args[0].tensorboard:\n",
        "#     a = (hyper_parameters,\n",
        "#          dict(zip(['z_precision', 'z_recall', 'z_f1', 'z_accuracy', 'z_ep'], best_metrics[-4:])))\n",
        " \n",
        "#     for j in hparams(*a):\n",
        "#         tb.file_writer.add_summary(j)\n",
        " \n",
        "#     for k, v in a[1].items():\n",
        "#         tb.add_scalar(k, v)\n",
        " \n",
        "#     tb.close()\n",
        " \n",
        "# %%"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Namespace(batch=8, dataset='', device='cuda', epochs=100, grad_clip=5.0, lr=2e-08, max_len=140, notes=None, ntransformer='Albert', num_classes=2, seed=1, tensorboard=True, weight=True), ['-f', '/root/.local/share/jupyter/runtime/kernel-e3fb01a7-5af2-4d1c-9308-e812dd3219d6.json'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-fdd6c4e3dceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0mlabel_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m \u001b[0mtrain_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m78\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HateEval/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0mtest_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-fdd6c4e3dceb>\u001b[0m in \u001b[0;36mdf2dl\u001b[0;34m(df, MAX_LEN_N)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"[CLS] \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' [SEP]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mtokenized_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mMAX_LEN_N\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     token_ids_matrix = np.array(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-fdd6c4e3dceb>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"[CLS] \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' [SEP]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mtokenized_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mMAX_LEN_N\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     token_ids_matrix = np.array(\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'tokenize'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUbokxB7Vekr"
      },
      "source": [
        "# !python main.py --dataset HateEval --num_classes 2 --epochs 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOc6nw4_hrBZ"
      },
      "source": [
        "# !python main_mt.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FgnfbncAWaI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "f84f3efb-b66a-4b6f-8786-b495d4444b27"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import datetime, time\n",
        "from shutil import rmtree\n",
        "\n",
        "from torch.utils import tensorboard\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.tensorboard.summary import hparams\n",
        "import transformers\n",
        "import tqdm\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# %%\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--ntransformer', default='Albert')\n",
        "\n",
        "parser.add_argument('--device', default='cuda')\n",
        "parser.add_argument('--weight', action='store_false')\n",
        "parser.add_argument('--lr', default=2e-8, type=float)\n",
        "parser.add_argument('--grad_clip', default=5., type=float)\n",
        "parser.add_argument('--seed', default=1, type=int)\n",
        "parser.add_argument('--notes', default=None)\n",
        "args = parser.parse_known_args()\n",
        "print(args)\n",
        "\n",
        "# %%\n",
        "# MAX_LEN_H = 140\n",
        "# MAX_LEN_O = 78\n",
        "\n",
        "MAX_LEN = 60\n",
        "N_EPOCHS = 200\n",
        "# N_EPOCHS = 1\n",
        "\n",
        "BATCH_SIZE = 24\n",
        "\n",
        "EPSILON = 10e-13\n",
        "LR = args[0].lr\n",
        "WEIGHT = args[0].weight\n",
        "# print('moh',WEIGHT)\n",
        "GRAD_CLIP = args[0].grad_clip\n",
        "device = torch.device(args[0].device) if torch.cuda.is_available() else torch.device('cpu')\n",
        "TRANSFORMER = args[0].ntransformer\n",
        "notes = args[0].notes\n",
        "\n",
        "SEED = args[0].seed\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "hyper_parameters = {'batch': BATCH_SIZE, 'lr': LR, 'weight': WEIGHT, 'transformer': TRANSFORMER, 'seed': SEED}\n",
        "\n",
        "# %%\n",
        "if TRANSFORMER == 'Albert':\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-en\")\n",
        "    transformer = AutoModelForMaskedLM.from_pretrained(\"nghuyong/ernie-2.0-en\")\n",
        "\n",
        "\n",
        "    #tokenizer = transformers.AlbertTokenizer.from_pretrained('albert-base-v2', do_lower_case=True)\n",
        "    #transformer = transformers.AlbertModel.from_pretrained(\"albert-base-v2\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "elif TRANSFORMER == 'DistilBert':\n",
        "    tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
        "    transformer = transformers.DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# %%\n",
        "\n",
        "# time_string = (datetime.datetime.utcnow() + datetime.timedelta(seconds=12600)).replace(microsecond=0).isoformat('_')\n",
        "# hyper_parameters_string = '--'.join(\n",
        "#     [k + '=' + str(v) for k, v in hyper_parameters.items()])\n",
        "# logdir = f'single_runs/{args[0].dataset}/{hyper_parameters_string}'\n",
        "# # if notes is not None:\n",
        "# #     logdir += notes\n",
        "# if os.path.exists(logdir):\n",
        "#     # rmtree(logdir)\n",
        "#     logdir += '/' + time_string\n",
        "# tb = tensorboard.SummaryWriter(log_dir=logdir)\n",
        "\n",
        "#%%\n",
        "\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "# class CustomBCELoss(nn.Module):\n",
        "#     def __init__(self):\n",
        "#       self.sigmas =\n",
        "\n",
        "\n",
        "def df2dl(df,max_len, batch_size, shuffle):\n",
        "    # if hate==True:\n",
        "    #   max_len = 140\n",
        "    # else:\n",
        "    #   max_len = 78\n",
        "    df = df.dropna()\n",
        "    df.text = df.text.apply(lambda x: \"[CLS] \" + x + ' [SEP]')\n",
        "    tokenized_texts = df.text.apply(lambda x: tokenizer.tokenize(' '.join(x.split()[:max_len])))\n",
        "    token_ids = tokenized_texts.apply(tokenizer.convert_tokens_to_ids)\n",
        "    token_ids_matrix = np.array(\n",
        "        token_ids.apply(lambda x: x[:max_len] + [0] * max(0, max_len - len(x))).tolist()).astype(\n",
        "        'int64')\n",
        "    attention_matrix = (token_ids_matrix != 0).astype('float')\n",
        "    dataset = TensorDataset(torch.tensor(token_ids_matrix), torch.tensor(attention_matrix),\n",
        "                            torch.tensor(np.array(df.label)))\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "dataset_names = 'OffenseEval HateEval Sentiment'.split(' ')\n",
        "num_datasets = len(dataset_names)\n",
        "# datasets = [dict() for _ in range(len(dataset_names))]\n",
        "\n",
        "train_data_loaders = []\n",
        "test_data_loaders = []\n",
        "criterions = []\n",
        "dl_num_classes = []\n",
        "\n",
        "for d_index, d in enumerate(dataset_names):\n",
        "    df = pd.read_csv(f'{d}/train.csv')\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    label_counts = df.label.value_counts(normalize=True)\n",
        "    train_weights = (1 / label_counts)\n",
        "    train_weights = train_weights.to_numpy() / train_weights.sum()\n",
        "    train_weights = torch.Tensor(train_weights).to(device)\n",
        "    # datasets[d_index]['tw'] = train_weights\n",
        "\n",
        "    if d == \"OffenseEval\":\n",
        "      max_len = 78\n",
        "    elif d == \"HateEval\":\n",
        "      max_len = 140\n",
        "    else:\n",
        "      max_len = 120\n",
        "      \n",
        "    train_data_loader = df2dl(df, max_len, BATCH_SIZE, True)\n",
        "\n",
        "    train_data_loaders.append(train_data_loader)\n",
        "\n",
        "\n",
        "    df = pd.read_csv(f'{d}/test.csv')\n",
        "\n",
        "    # if d == \"OffenseEval\":\n",
        "    #   test_data_loader = df2dl(df, 78, BATCH_SIZE, False)\n",
        "    # else:\n",
        "    #   test_data_loader = df2dl(df, 120, BATCH_SIZE, False)\n",
        "\n",
        "    test_data_loader = df2dl(df, 120, BATCH_SIZE, False)\n",
        "\n",
        "    test_data_loaders.append(test_data_loader)\n",
        "\n",
        "    num_classes = {\n",
        "      'Sentiment':2,\n",
        "      'OffenseEval': 2, \n",
        "      'HateEval': 2}[d]\n",
        "    dl_num_classes.append(num_classes)\n",
        "\n",
        "    if WEIGHT:\n",
        "        # print(WEIGHT)\n",
        "        # print('where you are')\n",
        "        if num_classes > 2:\n",
        "            criterion = nn.CrossEntropyLoss(reduction='sum', weight=train_weights)\n",
        "        else:\n",
        "            criterion = nn.BCEWithLogitsLoss(reduction='sum', pos_weight=train_weights[1])\n",
        "    else:\n",
        "        # print('where i am')\n",
        "        if num_classes > 2:\n",
        "            criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "        else:\n",
        "            criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "\n",
        "    criterions.append(criterion)\n",
        "\n",
        "#%%\n",
        "#کاری که میکنه اینکه \n",
        "#اول میشماره چنتا بچ داخل هر دیتالودر هستش\n",
        "#\n",
        "def shuffle_data_loaders(dataloaders):\n",
        "\n",
        "    batch_counts = [len(dl) for dl in dataloaders]\n",
        "    dataloader_iters = [iter(dl) for dl in dataloaders]\n",
        "\n",
        "    dataloader_indices = []\n",
        "    for idx, count in enumerate(batch_counts):\n",
        "        dataloader_indices.extend([idx] * count)\n",
        "\n",
        "#injaro bayad dune dune konim\n",
        "    random.shuffle(dataloader_indices)\n",
        "\n",
        "    for index in dataloader_indices:\n",
        "        yield next(dataloader_iters[index]), index, criterions[index], dl_num_classes[index]\n",
        "\n",
        "\n",
        "# %%\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.transformer = transformer\n",
        "        self.ffs = nn.ModuleList([nn.Sequential(\n",
        "            nn.Linear(30522, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(1000, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, nc if nc > 2 else 1)\n",
        "        ) for nc in dl_num_classes])\n",
        "\n",
        "    def forward(self, batch_input, batch_mask, task_id):\n",
        "        xx = self.transformer(batch_input, batch_mask)\n",
        "        xx = self.ffs[task_id](xx[0][:, 0, :]).squeeze()\n",
        "        return xx\n",
        "\n",
        "\n",
        "model = MyModel()\n",
        "model = model.to(device)\n",
        "# %%\n",
        "# param_optimizer = list(model.named_parameters())\n",
        "# no_decay = ['bias', 'gamma', 'beta']\n",
        "# optimizer_grouped_parameters = [\n",
        "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "#      'weight_decay_rate': 0.01},\n",
        "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "#      'weight_decay_rate': 0.0}\n",
        "# ]\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "# %%\n",
        "def calc_confusion_matrix(y, y_hat, num_classes):\n",
        "    with torch.no_grad():\n",
        "        if num_classes > 2:\n",
        "            pred = y_hat.argmax(-1)\n",
        "        else:\n",
        "            pred = y_hat.gt(0)\n",
        "\n",
        "    m = confusion_matrix(y.cpu(), pred.cpu(), range(num_classes))\n",
        "    return m\n",
        "\n",
        "\n",
        "def calc_metrics(m):\n",
        "    p = (m.diagonal() / m.sum(0).clip(EPSILON)).mean()\n",
        "    r = (m.diagonal() / m.sum(1).clip(EPSILON)).mean()\n",
        "    f1 = ((2 * p * r) / (p + r)).mean()\n",
        "    accu = m.diagonal().sum() / m.sum()\n",
        "    return p, r, f1, accu\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "metrics_all = []\n",
        "hate_f_one=[]\n",
        "hate_accuracy=[]\n",
        "\n",
        "sent_f_one=[]\n",
        "sent_accuracy=[]\n",
        "\n",
        "offe_f_one=[]\n",
        "offe_accuracy=[]\n",
        "\n",
        "\n",
        "# class MultiLossLayer():\n",
        "#   def __init__(self, loss_list):\n",
        "#     self._loss_list = loss_list\n",
        "#     self._sigmas_sq = []\n",
        "#     for i in range(len(self._loss_list)):\n",
        "#       self._sigmas_sq.append(slim.variable('Sigma_sq_' + str(i), dtype=tf.float32, shape=[], initializer=tf.initializers.random_uniform(minval=0.2, maxval=1)))\n",
        "\n",
        "#   def get_loss(self):\n",
        "#     factor = tf.div(1.0, tf.multiply(2.0, self._sigmas_sq[0]))\n",
        "#     loss = tf.add(tf.multiply(factor, self._loss_list[0]), tf.log(self._sigmas_sq[0]))\n",
        "#     for i in range(1, len(self._sigmas_sq)):\n",
        "#       factor = tf.div(1.0, tf.multiply(2.0, self._sigmas_sq[i]))\n",
        "#       loss = tf.add(loss, tf.add(tf.multiply(factor, self._loss_list[i]), tf.log(self._sigmas_sq[i])))\n",
        "#     return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_desc_string = '[ EP {0:02d} TRN ]' +\n",
        "                    ''.join([f\"[ {dataset_names[i][:4]} LS: {{{1 + 5 * i}:.3f}} \\\n",
        "                    F1: {{{4 + 5 * i}:.3f}} AC: {{{5 + 5 * i }:.3f}} ]\" for i in range(num_datasets)])\n",
        "flag=True\n",
        "\n",
        "# sigma_one = Variable(Uniform(torch.tensor([0.2]), torch.tensor([1.0])))\n",
        "\n",
        "\n",
        "# sigma_two = Variable(Uniform(torch.tensor([0.2]), torch.tensor([1.0])))\n",
        "  \n",
        "\n",
        "for i_epoch in range(1, N_EPOCHS + 1):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # for d_index, dataset in enumerate(datasets):\n",
        "    #     print(dataset_names[d_index])\n",
        "    # train_data_loader = dataset['train']\n",
        "    # criterion = dataset['criterion']\n",
        "    # num_classes = dataset['num_classes']\n",
        "    train_loss = [0] * num_datasets\n",
        "    train_confusion_table = [np.zeros((n, n)) for n in dl_num_classes]\n",
        "    train_total = [0] * num_datasets\n",
        "\n",
        "    p_bar = tqdm(shuffle_data_loaders(train_data_loaders), total=sum(len(dl) for dl in train_data_loaders))\n",
        "    # print('--'*2,p_bar)\n",
        "\n",
        "    metrics_train = np.zeros((5 * num_datasets,))\n",
        "\n",
        "    for step, stuff in enumerate(p_bar):\n",
        "        batch, d_index, criterion, num_classes = stuff\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        batch_input, batch_mask, batch_y = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(batch_input, batch_mask, d_index)\n",
        "        labels = batch_y if num_classes > 2 else batch_y.float()\n",
        "\n",
        "        # print('*'*5, labels)\n",
        "        # for i in range(1, len(self._sigmas_sq)):\n",
        "        #   factor = tf.div(1.0, tf.multiply(2.0, self._sigmas_sq[i]))\n",
        "        #   loss = tf.add(loss, tf.add(tf.multiply(factor, self._loss_list[i]), tf.log(self._sigmas_sq[i])))\n",
        "        # part_one = (1/(2*sigma_one))\n",
        "\n",
        "        loss = criterion(logits, batch_y if num_classes > 2 else batch_y.float())\n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        assert grad_norm >= 0, 'encounter nan in gradients.'\n",
        "        optimizer.step()\n",
        "\n",
        "        train_confusion_table[d_index] += calc_confusion_matrix(batch_y, logits, num_classes)\n",
        "        train_total[d_index] += batch_input.size(0)\n",
        "        train_loss[d_index] += loss.item()\n",
        "\n",
        "        metrics_train[d_index * 5: (d_index + 1) * 5] = np.array([\n",
        "            train_loss[d_index] / train_total[d_index], *calc_metrics(train_confusion_table[d_index]),\n",
        "        ])\n",
        "\n",
        "        p_bar.set_description(train_desc_string.format(i_epoch, *metrics_train))\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Set our model to testing mode (as opposed to evaluation mode)\n",
        "        model.eval()\n",
        "\n",
        "        for d_index, (test_data_loader, num_classes) in enumerate(zip(test_data_loaders, dl_num_classes)):\n",
        "\n",
        "            # Tracking variables\n",
        "            test_confusion_table = np.zeros((num_classes, num_classes))\n",
        "            test_total = 0\n",
        "            # test the data for one epoch\n",
        "            p_bar = tqdm(test_data_loader)\n",
        "            for step, batch in enumerate(p_bar):\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                batch_input, batch_mask, batch_y = batch\n",
        "\n",
        "                logits = model(batch_input, batch_mask, d_index)\n",
        "\n",
        "                test_confusion_table += calc_confusion_matrix(batch_y, logits, num_classes)\n",
        "                test_total += batch_input.size(0)\n",
        "                # test_loss += loss.item()\n",
        "\n",
        "                metrics_test = (\n",
        "                    # test_loss / test_total,\n",
        "                    *calc_metrics(test_confusion_table),\n",
        "                )\n",
        "\n",
        "                p_bar.set_description(\n",
        "                    '[ EP {0:02d} TST ]'\n",
        "                    '[ {1} F1: {4:.4f} AC: {5:.4f}]'\n",
        "                        .format(i_epoch, dataset_names[d_index][:4], *metrics_test))\n",
        "                \n",
        "\n",
        "            if (dataset_names[d_index][:4]=='Offe'):\n",
        "              offe_f_one.append(metrics_test[2])\n",
        "              offe_accuracy.append(metrics_test[3])\n",
        "            elif (dataset_names[d_index][:4]=='Hate'):\n",
        "              hate_f_one.append(metrics_test[2])\n",
        "              hate_accuracy.append(metrics_test[3])\n",
        "            else:\n",
        "              sent_f_one.append(metrics_test[2])\n",
        "              sent_accuracy.append(metrics_test[3])\n",
        "\n",
        "            if (len(hate_f_one)>20 and len(offe_f_one)>20 and len(sent_f_one)>20):\n",
        "\n",
        "              if( hate_f_one[-1]<hate_f_one[-21] and \n",
        "                 offe_f_one[-1]<offe_f_one[-21] and\n",
        "                 sent_f_one[-1]<sent_f_one[-21]):\n",
        "                \n",
        "                offe_index = offe_f_one.index(max(offe_f_one))\n",
        "                hate_index = hate_f_one.index(max(hate_f_one))\n",
        "                sent_index = sent_f_one.index(max(sent_f_one))\n",
        "\n",
        "                print(\"\\nearly stopping at {}\\n \\\n",
        "                        Offensive max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\\n \\\n",
        "                        Hate max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\\n \\\n",
        "                        Sent max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\".format(i_epoch , \n",
        "                            max(offe_f_one) , offe_accuracy[offe_index], offe_index+1, \n",
        "                            max(hate_f_one) , hate_accuracy[hate_index], hate_index+1, \n",
        "                            max(sent_f_one) , sent_accuracy[sent_index], sent_index+1))\n",
        "                \n",
        "                a = (\"\\nearly stopping at {}\\n \\\n",
        "                        Offensive max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\\n \\\n",
        "                        Hate max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\\n \\\n",
        "                        Sent max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\".format(i_epoch , \n",
        "                            max(offe_f_one) , offe_accuracy[offe_index], offe_index+1, \n",
        "                            max(hate_f_one) , hate_accuracy[hate_index], hate_index+1, \n",
        "                            max(sent_f_one) , sent_accuracy[sent_index], sent_index+1))\n",
        "                \n",
        "\n",
        "                with open(\"multiResult.txt\", 'a+') as writer:\n",
        "                    writer.write(a)\n",
        "                flag = False\n",
        "                break\n",
        "\n",
        "\n",
        "    if(not(flag)):\n",
        "      break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# %%"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-1dc0a78ab3f3>\"\u001b[0;36m, line \u001b[0;32m299\u001b[0m\n\u001b[0;31m    train_desc_string = '[ EP {0:02d} TRN ]' +\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7jZxp7DHl_z"
      },
      "source": [
        "            if(dataset_names[d_index][:4]=='Offe'):\n",
        "              offe_f_one.append(metrics_test[2])\n",
        "              offe_accuracy.append(metrics_test[3])\n",
        "            elif:\n",
        "              hate_f_one.append(metrics_test[2])\n",
        "              hate_accuracy.append(metrics_test[3])\n",
        "\n",
        "            else:\n",
        "              sent_f_one.append(metrics_test[2])\n",
        "              sent_accuracy.append(metrics_test[3])\n",
        "\n",
        "            if(len(hate_f_one)>20 and len(offe_f_one)>20 and len(sent_f_one)>20):\n",
        "\n",
        "              if( hate_f_one[-1]<hate_f_one[-21] and \n",
        "                 offe_f_one[-1]<offe_f_one[-21] and\n",
        "                 sent_f_one[-1]<sent_f_one[-21]):\n",
        "                \n",
        "                offe_index = offe_f_one.index(max(offe_f_one))\n",
        "                hate_index = hate_f_one.index(max(hate_f_one))\n",
        "                sent_index = sent_f_one.index(max(sent_f_one))\n",
        "\n",
        "                print(\"\\nearly stopping at {}\\n \\\n",
        "                        Offensive max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\\n \\\n",
        "                        Hate max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\\n \\\n",
        "                        Sent max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\".format(i_epoch , \n",
        "                            max(offe_f_one) , offe_accuracy[offe_index], offe_index+1, \n",
        "                            max(hate_f_one) , hate_accuracy[hate_index], hate_index+1, \n",
        "                            max(sent_f_one) , sent_accuracy[sent_index], sent_index+1))\n",
        "                \n",
        "                a = (\"\\nearly stopping at {}\\n \\\n",
        "                        Offensive max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\\n \\\n",
        "                        Hate max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\\n \\\n",
        "                        Sent max F1 is {:.4f} , max accuracy is {:.4f} in epoch {}\".format(i_epoch , \n",
        "                            max(offe_f_one) , offe_accuracy[offe_index], offe_index+1, \n",
        "                            max(hate_f_one) , hate_accuracy[hate_index], hate_index+1, \n",
        "                            max(sent_f_one) , sent_accuracy[sent_index], sent_index+1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfi8L_VrK_Cp"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "loss = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5fcHYyGKjCV"
      },
      "source": [
        "\n",
        "input = torch.tensor([ 0.0917,  0.0051,  0.1998,  0.0585,  0.0304,  0.0237,  0.0844,  0.2877,\n",
        "        -0.1244,  0.1348,  0.0177,  0.0103,  0.1387,  0.0503,  0.1195,  0.0242,\n",
        "         0.0085, -0.0014,  0.0990,  0.0315, -0.0265,  0.2276,  0.1223,  0.2129], requires_grad=True)\n",
        "target = torch.tensor([1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
        "        1., 1., 0., 1., 1., 1.])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "S_TiTUmKLHjA",
        "outputId": "498af61c-342b-40a1-be55-3517bec2e7f9"
      },
      "source": [
        "output = loss(input, target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e28165c659ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 962\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BujZvAXmLcwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be9a495-8f67-4de1-a616-27b6bf6bc581"
      },
      "source": [
        "a = [1, 2, 3]\n",
        "a[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpD99P3GSGKb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}