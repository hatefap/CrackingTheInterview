{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1nWeZQehPTS5e8LGxpeLEUOECV6HMDW--",
      "authorship_tag": "ABX9TyO+hzXzXOg7JBkq93sxnVWJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hatefap/CrackingTheInterview/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHZx2SzzhYd3",
        "outputId": "b6a5a069-8fe1-419a-a60f-018019d12016"
      },
      "source": [
        "!pip  install transformers numpy torch sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 25.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.8MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=1d090fc131502151e013fbafe43c0b35037ff1d0edc47de2f45f885b4c285328\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn0Grkw4hzyw"
      },
      "source": [
        "import torch\n",
        "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrYH42jXhsi6"
      },
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
        "    installed).\n",
        "\n",
        "    Args:\n",
        "        seed (:obj:`int`): The seed to set.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if is_torch_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # ^^ safe to call this function even if cuda is not available\n",
        "    if is_tf_available():\n",
        "        import tensorflow as tf\n",
        "\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "set_seed(101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYg5uNjPiHhx"
      },
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "# max sequence length for each document/sentence sample\n",
        "max_length = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX15WfHuiTgb"
      },
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-rrGR_wiX0-"
      },
      "source": [
        "farsi_path = \"drive/MyDrive/final_final.csv\"\n",
        "english_path = \"drive/MyDrive/my_final_english_hate.csv\"\n",
        "eng = pd.read_csv(english_path)\n",
        "farsi = pd.read_csv(farsi_path)\n",
        "eng.dropna(inplace=True)\n",
        "farsi.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRVFerEKi6F1"
      },
      "source": [
        "X_train = eng[['tweet']]\n",
        "Y_train = eng[['class']]\n",
        "\n",
        "X_google = farsi[['google']]\n",
        "Y_google = farsi[['ref3']]\n",
        "Y_google.columns = ['class']\n",
        "\n",
        "X_bart = farsi[['mbart']]\n",
        "Y_bart = farsi[['ref3']]\n",
        "Y_bart.columns = ['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLk07wkwjvlG"
      },
      "source": [
        "X_google_tune, X_google_test, Y_google_tune, Y_google_test = train_test_split(X_google, Y_google, test_size=0.4, random_state=101)\n",
        "\n",
        "X_mbart_tune, X_mbart_test  , Y_mbart_tune , Y_mbart_test  = train_test_split(X_bart, Y_bart ,test_size=0.4, random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "0TLkLUSQliM9",
        "outputId": "c6448594-f3ed-41d2-9fad-d949258bbcf3"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"@USER @USER @USER queer\" gaywad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"@USER @USER alsarabsss\" hes a beaner smh you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"@USER @USER you're fucking gay, blacklisted h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"@USER LMFAOOOO I HATE BLACK PEOPLE https://t....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"@USER \"At least I'm not a nigger\" http://t.co...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet\n",
              "0                   \"@USER @USER @USER queer\" gaywad\n",
              "1  \"@USER @USER alsarabsss\" hes a beaner smh you ...\n",
              "2  \"@USER @USER you're fucking gay, blacklisted h...\n",
              "3  \"@USER LMFAOOOO I HATE BLACK PEOPLE https://t....\n",
              "4  \"@USER \"At least I'm not a nigger\" http://t.co..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "IeIyoHqllNEl",
        "outputId": "c2926874-6d93-4fa0-bece-d1dd09e0d62c"
      },
      "source": [
        "X_google_tune.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>google</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4840</th>\n",
              "      <td>Mirage night was intercepted leave us\\nOur hea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2343</th>\n",
              "      <td>I recommend not to see horror films because I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4699</th>\n",
              "      <td>Jovin date and the number of tweets and follow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4228</th>\n",
              "      <td>Mr. Frdvsykhvany of Keykā'ūs and Afrasiab he c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4911</th>\n",
              "      <td>You are a child of my parents, I realized that...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 google\n",
              "4840  Mirage night was intercepted leave us\\nOur hea...\n",
              "2343  I recommend not to see horror films because I ...\n",
              "4699  Jovin date and the number of tweets and follow...\n",
              "4228  Mr. Frdvsykhvany of Keykā'ūs and Afrasiab he c...\n",
              "4911  You are a child of my parents, I realized that..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "J5_RiIHdlo3n",
        "outputId": "a3bfa5b4-b655-4250-e179-0527da4e7ad0"
      },
      "source": [
        "X_mbart_tune.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mbart</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4840</th>\n",
              "      <td>Leaving the night was our poison, the game was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2343</th>\n",
              "      <td>I recommend you don't watch a horror movie, be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4699</th>\n",
              "      <td>The history of June and the number of tweets a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4228</th>\n",
              "      <td>Mr. Iranian reading from Kafka and Arabic make...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4911</th>\n",
              "      <td>When I was a kid, the first difference I reali...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  mbart\n",
              "4840  Leaving the night was our poison, the game was...\n",
              "2343  I recommend you don't watch a horror movie, be...\n",
              "4699  The history of June and the number of tweets a...\n",
              "4228  Mr. Iranian reading from Kafka and Arabic make...\n",
              "4911  When I was a kid, the first difference I reali..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5AjJMRUkefP"
      },
      "source": [
        "# tokenize the dataset, truncate when passed `max_length`, \n",
        "# and pad with 0's when less than `max_length`\n",
        "train_encodings = tokenizer(X_train['tweet'].to_list(), truncation=True, padding=True, max_length=max_length)\n",
        "google_valid_encodings = tokenizer(X_google_tune['google'].to_list(), truncation=True, padding=True, max_length=max_length)\n",
        "mbart_valid_encodings = tokenizer(X_mbart_tune['mbart'].to_list(), truncation=True, padding=True, max_length=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrgmbWDSlGPP"
      },
      "source": [
        "class toTorchConverter(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# convert our tokenized data into a torch Dataset\n",
        "train_dataset = toTorchConverter(train_encodings, Y_train['class'].to_list())\n",
        "google_dataset_valid = toTorchConverter(google_valid_encodings, Y_google_tune['class'].astype(dtype=\"long\").to_list())\n",
        "bart_dataset_valid = toTorchConverter(mbart_valid_encodings, Y_mbart_tune['class'].astype(dtype=\"long\").to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfiPz5RtmuYb",
        "outputId": "6879c142-3842-45c5-d730-e42b9b1811e5"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPzoVKwJ5I88",
        "outputId": "8eaf2790-793f-42c5-845d-c28af871a558"
      },
      "source": [
        "model2 = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40aMFfRym8GM"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  # calculate accuracy using sklearn's function\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA8S6lSCnzTD"
      },
      "source": [
        "training_args_b = TrainingArguments(\n",
        "    output_dir='./results_bart',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs_bart',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "    logging_steps=200,               # log & save weights each logging_steps\n",
        "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBXNoD18oj15"
      },
      "source": [
        "training_args_g = TrainingArguments(\n",
        "    output_dir='./results_google',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs_google',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "    logging_steps=200,               # log & save weights each logging_steps\n",
        "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6_JYKJpoJAk"
      },
      "source": [
        "trainer_google = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args_g,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=google_dataset_valid,          # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WENkpphioUka"
      },
      "source": [
        "trainer_bart = Trainer(\n",
        "    model=model2,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args_b,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=bart_dataset_valid,          # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "pnp4Zrr-obyK",
        "outputId": "61a7664d-dd65-41cb-eb57-4b78055cbe05"
      },
      "source": [
        "# train the model\n",
        "trainer_google.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='2037' max='2037' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2037/2037 43:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.600200</td>\n",
              "      <td>0.478021</td>\n",
              "      <td>0.789153</td>\n",
              "      <td>46.993600</td>\n",
              "      <td>66.307000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.432200</td>\n",
              "      <td>0.595690</td>\n",
              "      <td>0.714056</td>\n",
              "      <td>46.678300</td>\n",
              "      <td>66.755000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.414300</td>\n",
              "      <td>0.422082</td>\n",
              "      <td>0.853338</td>\n",
              "      <td>46.699500</td>\n",
              "      <td>66.724000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.358800</td>\n",
              "      <td>0.599565</td>\n",
              "      <td>0.809050</td>\n",
              "      <td>46.737500</td>\n",
              "      <td>66.670000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.330800</td>\n",
              "      <td>0.656672</td>\n",
              "      <td>0.782092</td>\n",
              "      <td>46.795700</td>\n",
              "      <td>66.587000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.352900</td>\n",
              "      <td>0.488534</td>\n",
              "      <td>0.855905</td>\n",
              "      <td>46.962800</td>\n",
              "      <td>66.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.305500</td>\n",
              "      <td>0.692879</td>\n",
              "      <td>0.771181</td>\n",
              "      <td>47.071100</td>\n",
              "      <td>66.198000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.235200</td>\n",
              "      <td>0.581584</td>\n",
              "      <td>0.821566</td>\n",
              "      <td>46.812200</td>\n",
              "      <td>66.564000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.204700</td>\n",
              "      <td>0.690081</td>\n",
              "      <td>0.811297</td>\n",
              "      <td>46.651700</td>\n",
              "      <td>66.793000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.191900</td>\n",
              "      <td>0.706315</td>\n",
              "      <td>0.816752</td>\n",
              "      <td>46.622800</td>\n",
              "      <td>66.834000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2037, training_loss=0.3404766381459899, metrics={'train_runtime': 2606.9442, 'train_samples_per_second': 0.781, 'total_flos': 7122235291148196.0, 'epoch': 3.0, 'init_mem_cpu_alloc_delta': 161338039, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 1384340, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 1660100, 'train_mem_gpu_alloc_delta': 1781358080, 'train_mem_cpu_peaked_delta': 94789511, 'train_mem_gpu_peaked_delta': 6275048448})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "orsYPyaD3DuW",
        "outputId": "560fed1f-7ac7-47c6-908f-5cf4308f546f"
      },
      "source": [
        "trainer_google.evaluate()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [156/156 00:38]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 3.0,\n",
              " 'eval_accuracy': 0.8533376123234917,\n",
              " 'eval_loss': 0.4220818877220154,\n",
              " 'eval_mem_cpu_alloc_delta': 124148,\n",
              " 'eval_mem_cpu_peaked_delta': 224693,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 186651648,\n",
              " 'eval_runtime': 39.1923,\n",
              " 'eval_samples_per_second': 79.505}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KloKlXLE4lb0",
        "outputId": "ca78d3af-0a21-4bac-8ec1-9be98a8fe1a4"
      },
      "source": [
        "model_path = \"drive/MyDrive/google-bert-base-uncased\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('drive/MyDrive/google-bert-base-uncased/tokenizer_config.json',\n",
              " 'drive/MyDrive/google-bert-base-uncased/special_tokens_map.json',\n",
              " 'drive/MyDrive/google-bert-base-uncased/vocab.txt',\n",
              " 'drive/MyDrive/google-bert-base-uncased/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "x3hep6C-o9Qh",
        "outputId": "1bc0b0b4-2548-4a1a-a5ad-501250bae2b8"
      },
      "source": [
        "trainer_bart.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='2037' max='2037' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2037/2037 42:02, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.122600</td>\n",
              "      <td>0.768254</td>\n",
              "      <td>0.815469</td>\n",
              "      <td>40.399800</td>\n",
              "      <td>77.129000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.122500</td>\n",
              "      <td>0.768254</td>\n",
              "      <td>0.815469</td>\n",
              "      <td>41.745500</td>\n",
              "      <td>74.643000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.120700</td>\n",
              "      <td>0.768254</td>\n",
              "      <td>0.815469</td>\n",
              "      <td>41.980200</td>\n",
              "      <td>74.225000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.126300</td>\n",
              "      <td>0.768254</td>\n",
              "      <td>0.815469</td>\n",
              "      <td>42.172100</td>\n",
              "      <td>73.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.126700</td>\n",
              "      <td>0.768254</td>\n",
              "      <td>0.815469</td>\n",
              "      <td>42.301100</td>\n",
              "      <td>73.662000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.110100</td>\n",
              "      <td>0.768254</td>\n",
              "      <td>0.815469</td>\n",
              "      <td>42.099600</td>\n",
              "      <td>74.015000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.119500</td>\n",
              "      <td>0.768254</td>\n",
              "      <td>0.815469</td>\n",
              "      <td>41.868700</td>\n",
              "      <td>74.423000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.127800</td>\n",
              "      <td>0.768254</td>\n",
              "      <td>0.815469</td>\n",
              "      <td>41.771200</td>\n",
              "      <td>74.597000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.132800</td>\n",
              "      <td>0.768254</td>\n",
              "      <td>0.815469</td>\n",
              "      <td>42.473500</td>\n",
              "      <td>73.363000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.118000</td>\n",
              "      <td>0.768254</td>\n",
              "      <td>0.815469</td>\n",
              "      <td>42.491600</td>\n",
              "      <td>73.332000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2037, training_loss=0.1222490745838448, metrics={'train_runtime': 2523.9044, 'train_samples_per_second': 0.807, 'total_flos': 7122235291148196.0, 'epoch': 3.0, 'init_mem_cpu_alloc_delta': 49606, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 18306, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 1907143, 'train_mem_gpu_alloc_delta': 442944512, 'train_mem_cpu_peaked_delta': 94804109, 'train_mem_gpu_peaked_delta': 6297678848})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nNEuHjHNSpu",
        "outputId": "43146b17-1f2b-46b3-d95a-e131499f397a"
      },
      "source": [
        "model_path = \"drive/MyDrive/mbart-bert-base-uncased\"\n",
        "model2.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('drive/MyDrive/mbart-bert-base-uncased/tokenizer_config.json',\n",
              " 'drive/MyDrive/mbart-bert-base-uncased/special_tokens_map.json',\n",
              " 'drive/MyDrive/mbart-bert-base-uncased/vocab.txt',\n",
              " 'drive/MyDrive/mbart-bert-base-uncased/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiImDlaXDVsu"
      },
      "source": [
        "def get_prediction(text):\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = model(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    return probs.argmax().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z8Iz4hGPNA9"
      },
      "source": [
        "X_google_test_result = [get_prediction(item) for item in X_google_test['google'].to_list()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uLhOgA8PRB1",
        "outputId": "a4346ce4-951b-42a0-f80d-4dd8d9ae99c9"
      },
      "source": [
        "print(classification_report(Y_google_test, X_google_test_result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.90      0.90      1786\n",
            "         1.0       0.38      0.37      0.37       292\n",
            "\n",
            "    accuracy                           0.83      2078\n",
            "   macro avg       0.64      0.63      0.64      2078\n",
            "weighted avg       0.82      0.83      0.83      2078\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOaLmStARCTA"
      },
      "source": [
        "def get_prediction_mbart(text):\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = model2(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    return probs.argmax().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "mkkMdp05TRKl",
        "outputId": "5430abf1-a64b-4954-840a-043692280d9a"
      },
      "source": [
        "X_mbart_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mbart</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3722</th>\n",
              "      <td>Every time you tweeted for the Pantheon of ang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1875</th>\n",
              "      <td>I was talking about politics, and somebody I k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4419</th>\n",
              "      <td>There's been severe flooding and rainfall, and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3066</th>\n",
              "      <td>The world, don't cry to Hussein, it's not wort...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1641</th>\n",
              "      <td>Ali Zardari is a well-known gang member who ki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1258</th>\n",
              "      <td>I mean, it's so bad that the Iranian row is so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3101</th>\n",
              "      <td>In this interesting situation, the common deno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1262</th>\n",
              "      <td>A wise person can't be cut through a hole twic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2069</th>\n",
              "      <td>What is it that makes you jump out of bed and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>What's the voice of the workers? Is there a un...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2078 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  mbart\n",
              "3722  Every time you tweeted for the Pantheon of ang...\n",
              "1875  I was talking about politics, and somebody I k...\n",
              "4419  There's been severe flooding and rainfall, and...\n",
              "3066  The world, don't cry to Hussein, it's not wort...\n",
              "1641  Ali Zardari is a well-known gang member who ki...\n",
              "...                                                 ...\n",
              "1258  I mean, it's so bad that the Iranian row is so...\n",
              "3101  In this interesting situation, the common deno...\n",
              "1262  A wise person can't be cut through a hole twic...\n",
              "2069  What is it that makes you jump out of bed and ...\n",
              "322   What's the voice of the workers? Is there a un...\n",
              "\n",
              "[2078 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RA4rZwLS4jd"
      },
      "source": [
        "X_mbart_test_result = [get_prediction_mbart(item) for item in X_mbart_test['mbart'].to_list()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIwXvmOlSuX0",
        "outputId": "07832174-0232-45d4-937c-1faeded848c2"
      },
      "source": [
        "print(classification_report(Y_mbart_test, X_mbart_test_result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.94      0.91      1786\n",
            "         1.0       0.36      0.22      0.27       292\n",
            "\n",
            "    accuracy                           0.84      2078\n",
            "   macro avg       0.62      0.58      0.59      2078\n",
            "weighted avg       0.81      0.84      0.82      2078\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK91HQipTjHT",
        "outputId": "41359832-b513-40c3-f03c-e486229f62cc"
      },
      "source": [
        "model_without_english = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShcUrjAgW6k0"
      },
      "source": [
        "training_args_google_without_english = TrainingArguments(\n",
        "    output_dir='./results_google_without_english',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./results_google_without_english',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "    logging_steps=200,               # log & save weights each logging_steps\n",
        "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqKCr2CNXSR_"
      },
      "source": [
        "trainer_google_without_english = Trainer(\n",
        "    model=model_without_english,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args_google_without_english,                  # training arguments, defined above\n",
        "    train_dataset=google_dataset_valid,         # training dataset\n",
        "    eval_dataset=google_dataset_valid,\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnnC9IBCX6-E"
      },
      "source": [
        "!rm -rf ./results_bart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "jxLPdiMrXsJE",
        "outputId": "9420d916-91f1-41ea-8275-8f8728e0e3d7"
      },
      "source": [
        "trainer_google_without_english.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [585/585 07:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.261200</td>\n",
              "      <td>0.250724</td>\n",
              "      <td>0.929076</td>\n",
              "      <td>44.423400</td>\n",
              "      <td>70.143000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.279100</td>\n",
              "      <td>0.139554</td>\n",
              "      <td>0.957959</td>\n",
              "      <td>45.742700</td>\n",
              "      <td>68.120000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=585, training_loss=0.24613858084393364, metrics={'train_runtime': 477.6805, 'train_samples_per_second': 1.225, 'total_flos': 1344819024761616.0, 'epoch': 3.0, 'init_mem_cpu_alloc_delta': 302457, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 598788, 'init_mem_gpu_peaked_delta': 1329002496, 'train_mem_cpu_alloc_delta': 970382, 'train_mem_gpu_alloc_delta': 1356288512, 'train_mem_cpu_peaked_delta': 94077016, 'train_mem_gpu_peaked_delta': 3242928128})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "piqc8gmgYB9T",
        "outputId": "78fb41e1-66b7-4b7e-feb5-5e61d4845b29"
      },
      "source": [
        "trainer_google_without_english.evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [156/156 00:40]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 3.0,\n",
              " 'eval_accuracy': 0.95795892169448,\n",
              " 'eval_loss': 0.13955430686473846,\n",
              " 'eval_mem_cpu_alloc_delta': 115315,\n",
              " 'eval_mem_cpu_peaked_delta': 224651,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 919243264,\n",
              " 'eval_runtime': 40.7461,\n",
              " 'eval_samples_per_second': 76.474}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCD4PpdHa2tt"
      },
      "source": [
        "def get_prediction_google_without_english(text):\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = model_without_english(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    return probs.argmax().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV8EhvhmbQ8b"
      },
      "source": [
        "X_google_test_result_without_engish = [get_prediction_google_without_english(item) for item in X_google_test['google'].to_list()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_u6yiDybj7q",
        "outputId": "24bdfdfb-f527-4209-e758-f8124414c64e"
      },
      "source": [
        "print(classification_report(Y_google_test, X_google_test_result_without_engish))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.84      0.88      1786\n",
            "         1.0       0.39      0.63      0.49       292\n",
            "\n",
            "    accuracy                           0.81      2078\n",
            "   macro avg       0.66      0.74      0.69      2078\n",
            "weighted avg       0.86      0.81      0.83      2078\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig3XxSlJbu5x",
        "outputId": "37b8b649-8e5f-4972-b686-0017ad35b8c1"
      },
      "source": [
        "bart_without_english = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k1J1Hh1cDkZ"
      },
      "source": [
        "training_args_bart_without_english = TrainingArguments(\n",
        "    output_dir='./results_bart_without_english',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./results_bart_without_english',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "    logging_steps=200,               # log & save weights each logging_steps\n",
        "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cNw3BD3cMsA"
      },
      "source": [
        "trainer_bart_without_english = Trainer(\n",
        "    model=bart_without_english,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args_bart_without_english,                  # training arguments, defined above\n",
        "    train_dataset=bart_dataset_valid,         # training dataset\n",
        "    eval_dataset=bart_dataset_valid,\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "RVLxY_86ce8Y",
        "outputId": "c9c2a277-10f2-403e-8f1f-df51ad4a84de"
      },
      "source": [
        "trainer_bart_without_english.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [585/585 07:39, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.460100</td>\n",
              "      <td>0.349075</td>\n",
              "      <td>0.855905</td>\n",
              "      <td>42.334900</td>\n",
              "      <td>73.604000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.322800</td>\n",
              "      <td>0.157976</td>\n",
              "      <td>0.948652</td>\n",
              "      <td>42.221200</td>\n",
              "      <td>73.802000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=585, training_loss=0.33418382823976694, metrics={'train_runtime': 460.0286, 'train_samples_per_second': 1.272, 'total_flos': 1228145228092800.0, 'epoch': 3.0, 'init_mem_cpu_alloc_delta': 54913, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 18306, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 968379, 'train_mem_gpu_alloc_delta': 1767702528, 'train_mem_cpu_peaked_delta': 94080696, 'train_mem_gpu_peaked_delta': 2814068224})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "vKkhMvRLchyK",
        "outputId": "8e476e44-a264-4e8f-d8e3-408619d187d8"
      },
      "source": [
        "trainer_bart_without_english.evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [156/156 00:42]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 3.0,\n",
              " 'eval_accuracy': 0.9486521181001284,\n",
              " 'eval_loss': 0.15797649323940277,\n",
              " 'eval_mem_cpu_alloc_delta': 117884,\n",
              " 'eval_mem_cpu_peaked_delta': 228691,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 164764160,\n",
              " 'eval_runtime': 42.3901,\n",
              " 'eval_samples_per_second': 73.508}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a5VWrFacsWD"
      },
      "source": [
        "def get_prediction_bart_without_english(text):\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = bart_without_english(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    return probs.argmax().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvh24pqVcy44"
      },
      "source": [
        "X_bart_test_result_without_engish = [get_prediction_bart_without_english(item) for item in X_mbart_test['mbart'].to_list()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3PSbqCsc9I3",
        "outputId": "b1c1980c-e638-4ffb-c2e7-3d101f0f395b"
      },
      "source": [
        "print(classification_report(Y_mbart_test, X_bart_test_result_without_engish))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.89      0.90      1786\n",
            "         1.0       0.42      0.47      0.44       292\n",
            "\n",
            "    accuracy                           0.83      2078\n",
            "   macro avg       0.66      0.68      0.67      2078\n",
            "weighted avg       0.84      0.83      0.84      2078\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io17X2CVdGny",
        "outputId": "a9bea77d-4024-4732-f4bd-c62ecbdc5a19"
      },
      "source": [
        "model_path = \"drive/MyDrive/google_without_english-bert-base-uncased\"\n",
        "model_without_english.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('drive/MyDrive/google_without_english-bert-base-uncased/tokenizer_config.json',\n",
              " 'drive/MyDrive/google_without_english-bert-base-uncased/special_tokens_map.json',\n",
              " 'drive/MyDrive/google_without_english-bert-base-uncased/vocab.txt',\n",
              " 'drive/MyDrive/google_without_english-bert-base-uncased/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7f1uhKjfPs7",
        "outputId": "dd261b31-d71e-4cee-fe9c-5e29d2605f5d"
      },
      "source": [
        "model_path = \"drive/MyDrive/bart_without_english-bert-base-uncased\"\n",
        "bart_without_english.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('drive/MyDrive/bart_without_english-bert-base-uncased/tokenizer_config.json',\n",
              " 'drive/MyDrive/bart_without_english-bert-base-uncased/special_tokens_map.json',\n",
              " 'drive/MyDrive/bart_without_english-bert-base-uncased/vocab.txt',\n",
              " 'drive/MyDrive/bart_without_english-bert-base-uncased/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yy1JDrXfV1T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}