{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roberto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16I5KrtkDqd3Y5DffHI4QaUh8TxVb6MEm",
      "authorship_tag": "ABX9TyMfKR64FVvTJkTIIV1xnzfE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hatefap/CrackingTheInterview/blob/master/roberto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0HJ6boxLJVC",
        "outputId": "87e102d2-6622-4501-d7af-c5123cb9ba97"
      },
      "source": [
        "!pip  install transformers numpy torch sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 35.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=bfafe55e2899b4ca0653611d4584d67d2dd84dffb469ea9dcc267914b1b1f901\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqQHZSB8LRkK"
      },
      "source": [
        "import torch\n",
        "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
        "from transformers import XLMRobertaTokenizerFast, XLMRobertaForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvMTOMCaQVY4",
        "outputId": "bafcae44-26e1-4f70-b0dc-c416bd03e906"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjfnvcJjNzAT"
      },
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
        "    installed).\n",
        "\n",
        "    Args:\n",
        "        seed (:obj:`int`): The seed to set.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if is_torch_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # ^^ safe to call this function even if cuda is not available\n",
        "    if is_tf_available():\n",
        "        import tensorflow as tf\n",
        "\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "set_seed(101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g9cLUK0N4_a"
      },
      "source": [
        "model_name = \"xlm-roberta-base\"\n",
        "# max sequence length for each document/sentence sample\n",
        "max_length = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-636aMMtN_vI"
      },
      "source": [
        "tokenizer_xlmroberta = XLMRobertaTokenizerFast.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiOzuQCWOUls"
      },
      "source": [
        "farsi_path = \"drive/MyDrive/final_final.csv\"\n",
        "english_path = \"drive/MyDrive/my_final_english_hate.csv\"\n",
        "eng = pd.read_csv(english_path)\n",
        "farsi = pd.read_csv(farsi_path)\n",
        "eng.dropna(inplace=True)\n",
        "farsi.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O16hHqWDQtkR",
        "outputId": "e98a18f5-7d27-4fae-f136-7c04d707d1c4"
      },
      "source": [
        "farsi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>ref1</th>\n",
              "      <th>ref2</th>\n",
              "      <th>link</th>\n",
              "      <th>norm_text</th>\n",
              "      <th>google</th>\n",
              "      <th>mbart</th>\n",
              "      <th>ref3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>بچه های عزیز این کشتزار منو بلاک کرده یکی که د...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>https://twitter.com/DrErovay/status/1249849840...</td>\n",
              "      <td>بچه‌های عزیز این کشتزار منو بلاک کرده یکی که د...</td>\n",
              "      <td>Children's Menu block the field of view of one...</td>\n",
              "      <td>My dear children, I've been blocked on this fa...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>سیاسیون محترم از  نمایندگان گرفته تا  شهردارا...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>https://twitter.com/atefeh heidari/status/1007...</td>\n",
              "      <td>سیاسیون محترم از نمایندگان گرفته تا شهرداران ...</td>\n",
              "      <td>Distinguished delegates politicians to munici...</td>\n",
              "      <td>From respectable politicians to elected offici...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>با اعدام مخالفم \\n\\nکاش راهکاری برای نابودی و ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>https://twitter.com/fariborz c/status/10124857...</td>\n",
              "      <td>با اعدام مخالفم \\n\\nکاش راهکاری برای نابودی و ...</td>\n",
              "      <td>I disagree with execution\\n\\nIf only way to an...</td>\n",
              "      <td>With the execution of my opponent, I wish ther...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>دونوع آدم داریم : یکی اونکه هیچی به کیرش نیست ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>https://twitter.com/llil ili lill/status/10190...</td>\n",
              "      <td>دونوع آدم داریم: یکی اونکه هیچی به کیرش نیست و...</td>\n",
              "      <td>There are two types of people: one Avnkh nothi...</td>\n",
              "      <td>There are two types of people: There's the one...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>هيچ زنداني بدتر از ،دختر بودن، نيست....</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>https://twitter.com/virgoOoOol/status/10233572...</td>\n",
              "      <td>هیچ زندانی بدتر از، دختر بودن، نیست ….</td>\n",
              "      <td>No prisoners worse, the girl is not ....</td>\n",
              "      <td>There is no prison worse than being a girl...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5195</th>\n",
              "      <td>5196</td>\n",
              "      <td>من ایشون میشناسم تو اینستام پیج داره تا جایی ک...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>https://twitter.com/Naslsokhte1/status/1150193...</td>\n",
              "      <td>من ایشون میشناسم تو اینستام پیج داره تا جایی ک...</td>\n",
              "      <td>She got as far as I know you saw Paige Aynstam...</td>\n",
              "      <td>I know one of them. He's been on this team for...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5196</th>\n",
              "      <td>5197</td>\n",
              "      <td>پسرا، هم جنسا، چرا  دختر هرچی  بددهن باشه و کص...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>https://twitter.com/Merajsn/status/11501930125...</td>\n",
              "      <td>پسرا، هم جنسا، چرا دختر هرچی بددهن باشه و کصشع...</td>\n",
              "      <td>Boys, both generically, because the girl is ge...</td>\n",
              "      <td>Guys, same-sex, why would a girl be anything b...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5197</th>\n",
              "      <td>5198</td>\n",
              "      <td>هدایت و بازداشت شده است.\\nامارات سیاست سکوت را...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>https://twitter.com/MadariSarzamin/status/1150...</td>\n",
              "      <td>هدایت و بازداشت شده است. \\nامارات سیاست سکوت ر...</td>\n",
              "      <td>The guidance has been arrested.\\nEmirates has ...</td>\n",
              "      <td>The UAE has adopted a policy of silence, and s...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5198</th>\n",
              "      <td>5199</td>\n",
              "      <td>عزیزان، کسی گفته کاربران توئیتر مستجاب الدعوه ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>https://twitter.com/LeylaNarimani/status/11509...</td>\n",
              "      <td>عزیزان، کسی گفته کاربران توئیتر مستجاب الدعوه‌...</td>\n",
              "      <td>Loved ones, who have said Twitter users answer...</td>\n",
              "      <td>Dear friends, has anyone said that Twitter use...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5199</th>\n",
              "      <td>5200</td>\n",
              "      <td>میگن هیچ چیزی بدون دلیل نیست\\nهمه چی دست به دس...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>https://twitter.com/themifar/status/1150917824...</td>\n",
              "      <td>میگن هیچ چیزی بدون دلیل نیست\\nهمه چی دست به دس...</td>\n",
              "      <td>They say nothing is not without reason\\nEveryt...</td>\n",
              "      <td>They say nothing goes without a reason, everyt...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5194 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... ref3\n",
              "0        0  ...  1.0\n",
              "1        1  ...  0.0\n",
              "2        2  ...  1.0\n",
              "3        3  ...  0.0\n",
              "4        4  ...  0.0\n",
              "...    ...  ...  ...\n",
              "5195  5196  ...  0.0\n",
              "5196  5197  ...  0.0\n",
              "5197  5198  ...  0.0\n",
              "5198  5199  ...  0.0\n",
              "5199  5200  ...  0.0\n",
              "\n",
              "[5194 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUzu8V_iQgK-"
      },
      "source": [
        "X_train = eng[['tweet']]\n",
        "Y_train = eng[['class']]\n",
        "\n",
        "\n",
        "X_farsi = farsi[['text']]\n",
        "Y_farsi = farsi[['ref3']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPHB-MXBRImj"
      },
      "source": [
        "X_tune, X_test, Y_tune, Y_test = train_test_split(X_farsi, Y_farsi, test_size=0.4, random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5UMiKbPRWwq"
      },
      "source": [
        "# tokenize the dataset, truncate when passed `max_length`, \n",
        "# and pad with 0's when less than `max_length`\n",
        "train_encodings = tokenizer_xlmroberta(X_train['tweet'].to_list(), truncation=True, padding=True, max_length=max_length)\n",
        "farsi_valid_encodings = tokenizer_xlmroberta(X_tune['text'].to_list(), truncation=True, padding=True, max_length=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GDoQ7OLRyq9"
      },
      "source": [
        "class toTorchConverter(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# convert our tokenized data into a torch Dataset\n",
        "train_dataset = toTorchConverter(train_encodings, Y_train['class'].to_list())\n",
        "farsi_dataset_valid = toTorchConverter(farsi_valid_encodings, Y_tune['ref3'].astype(dtype=\"long\").to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBHikqsXT_kl",
        "outputId": "75c682c4-f905-4799-8e0e-7b51e4354fc6"
      },
      "source": [
        "model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=2).to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlJ0ZFCLho3N"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  # calculate accuracy using sklearn's function\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQzblGBzhvSr"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_roberto',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs_roberto',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "    logging_steps=400,               # log & save weights each logging_steps\n",
        "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJjRxlYkiHcK"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=farsi_dataset_valid,          # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "MwnY8neoiahz",
        "outputId": "2a0af93a-1d92-4637-cbfa-415789c9563b"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='2037' max='2037' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2037/2037 39:18, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.569300</td>\n",
              "      <td>1.129233</td>\n",
              "      <td>0.537548</td>\n",
              "      <td>33.916200</td>\n",
              "      <td>91.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.458100</td>\n",
              "      <td>0.475772</td>\n",
              "      <td>0.839859</td>\n",
              "      <td>33.901500</td>\n",
              "      <td>91.913000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.428600</td>\n",
              "      <td>0.531694</td>\n",
              "      <td>0.844031</td>\n",
              "      <td>33.770700</td>\n",
              "      <td>92.269000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.402500</td>\n",
              "      <td>0.499226</td>\n",
              "      <td>0.814827</td>\n",
              "      <td>33.803900</td>\n",
              "      <td>92.179000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.345700</td>\n",
              "      <td>0.586893</td>\n",
              "      <td>0.739089</td>\n",
              "      <td>33.977900</td>\n",
              "      <td>91.707000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2037, training_loss=0.43913782866023365, metrics={'train_runtime': 2359.4932, 'train_samples_per_second': 0.863, 'total_flos': 1.6077902822689824e+16, 'epoch': 3.0, 'init_mem_cpu_alloc_delta': 90651499, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 1041335, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 1592921, 'train_mem_gpu_alloc_delta': 4497489408, 'train_mem_cpu_peaked_delta': 768919005, 'train_mem_gpu_peaked_delta': 4501180416})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "nplwPHFHieqk",
        "outputId": "4093f91d-311e-46c2-bde2-62cc554d77a5"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [156/156 00:32]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 3.0,\n",
              " 'eval_accuracy': 0.8398587933247753,\n",
              " 'eval_loss': 0.4757724106311798,\n",
              " 'eval_mem_cpu_alloc_delta': 114656,\n",
              " 'eval_mem_cpu_peaked_delta': 231069,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 135332352,\n",
              " 'eval_runtime': 32.7333,\n",
              " 'eval_samples_per_second': 95.194}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP_PpS1gzAfk",
        "outputId": "ee2b8ba5-3909-412c-e2e3-9fc0497f86f4"
      },
      "source": [
        "model_path = \"drive/MyDrive/xlmr\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer_xlmroberta.save_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('drive/MyDrive/xlmr/tokenizer_config.json',\n",
              " 'drive/MyDrive/xlmr/special_tokens_map.json',\n",
              " 'drive/MyDrive/xlmr/sentencepiece.bpe.model',\n",
              " 'drive/MyDrive/xlmr/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5cQUVu_zGuD"
      },
      "source": [
        "def get_prediction(text):\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer_xlmroberta(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = model(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    return probs.argmax().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JydxBmLp3yri"
      },
      "source": [
        "X_test_result = [get_prediction(item) for item in X_test['text'].to_list()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_NJjZ9X4ldZ",
        "outputId": "db13b32b-cc96-462b-8ed5-5e604dd257ab"
      },
      "source": [
        "print(classification_report(Y_test, X_test_result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.83      0.86      1786\n",
            "         1.0       0.24      0.32      0.27       292\n",
            "\n",
            "    accuracy                           0.76      2078\n",
            "   macro avg       0.56      0.58      0.57      2078\n",
            "weighted avg       0.79      0.76      0.78      2078\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfyrXjsE4ygO",
        "outputId": "c27e1874-dd86-48d4-f9af-167f1dfb8255"
      },
      "source": [
        "model_without_english = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=2).to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5bp5yiu6r9c"
      },
      "source": [
        "training_args_without_english = TrainingArguments(\n",
        "    output_dir='./xlmr_without_english',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./xlmr_without_english_logs',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "    logging_steps=400,               # log & save weights each logging_steps\n",
        "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVzSXOIf7xgS"
      },
      "source": [
        "trainer_without_english = Trainer(\n",
        "    model=model_without_english,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args_without_english,                  # training arguments, defined above\n",
        "    train_dataset=farsi_dataset_valid,         # training dataset\n",
        "    eval_dataset=farsi_dataset_valid,\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lEExTAu9zI-"
      },
      "source": [
        "!rm -rf ./results_roberto/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "Uyk57a4197ie",
        "outputId": "7976cedb-2813-4310-bdd7-b603462a4bab"
      },
      "source": [
        "trainer_without_english.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [585/585 07:52, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.431100</td>\n",
              "      <td>0.342772</td>\n",
              "      <td>0.882221</td>\n",
              "      <td>34.001900</td>\n",
              "      <td>91.642000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=585, training_loss=0.39083526480911124, metrics={'train_runtime': 472.9147, 'train_samples_per_second': 1.237, 'total_flos': 2744719717056768.0, 'epoch': 3.0, 'init_mem_cpu_alloc_delta': 49480, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 18274, 'init_mem_gpu_peaked_delta': 1112325632, 'train_mem_cpu_alloc_delta': 1111296, 'train_mem_gpu_alloc_delta': 4476404224, 'train_mem_cpu_peaked_delta': 768213493, 'train_mem_gpu_peaked_delta': 1620997120})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "VY6PEAEn-Aqe",
        "outputId": "6955726f-c911-4746-e6fc-f77cc49b0e04"
      },
      "source": [
        "trainer_without_english.evaluate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [156/156 00:32]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 3.0,\n",
              " 'eval_accuracy': 0.8822207958921694,\n",
              " 'eval_loss': 0.34277206659317017,\n",
              " 'eval_mem_cpu_alloc_delta': 120538,\n",
              " 'eval_mem_cpu_peaked_delta': 226341,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 135332352,\n",
              " 'eval_runtime': 32.7263,\n",
              " 'eval_samples_per_second': 95.214}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUrpmpd0CK-a"
      },
      "source": [
        "def get_prediction_wighout_english(text):\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer_xlmroberta(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = model_without_english(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    return probs.argmax().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2QmkmVQBX9L",
        "outputId": "47eab3cf-9e9f-46fb-db27-1a60162843d4"
      },
      "source": [
        "X_test_result = [get_prediction_wighout_english(item) for item in X_test['text'].to_list()]\n",
        "\n",
        "print(classification_report(Y_test, X_test_result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.97      0.94      1786\n",
            "         1.0       0.68      0.43      0.53       292\n",
            "\n",
            "    accuracy                           0.89      2078\n",
            "   macro avg       0.79      0.70      0.73      2078\n",
            "weighted avg       0.88      0.89      0.88      2078\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t29p05PIA-ZL",
        "outputId": "e9849ad4-a3f2-4ac5-940a-434cc66f1923"
      },
      "source": [
        "model_path = \"drive/MyDrive/xlmr_withoutout_english\"\n",
        "model_without_english.save_pretrained(model_path)\n",
        "tokenizer_xlmroberta.save_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('drive/MyDrive/xlmr_withoutout_english/tokenizer_config.json',\n",
              " 'drive/MyDrive/xlmr_withoutout_english/special_tokens_map.json',\n",
              " 'drive/MyDrive/xlmr_withoutout_english/sentencepiece.bpe.model',\n",
              " 'drive/MyDrive/xlmr_withoutout_english/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOUXXM9TBS4S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}